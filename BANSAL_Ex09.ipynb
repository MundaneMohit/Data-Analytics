{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Course: Distributed Data Analytics Exercise Sheet 9\n",
    "**Submitted by: Mohit Bansal**\n",
    "\n",
    "**Student ID: 279314**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Apache Spark Basics ( 10 points)\n",
    "\n",
    "#### Part a) Basic Operations on Resilient Distributed Dataset (RDD) (4 points)\n",
    "\n",
    "Let’s have two lists of words as follows:\n",
    "\n",
    "    a = [\"spark\", \"rdd\", \"python\", \"context\", \"create\", \"class\"]\n",
    "\n",
    "    b = [\"operation\", \"apache\", \"scala\", \"lambda\",\"parallel\",\"partition\"]\n",
    "\n",
    "Create two RDD objects of a, b and do the following tasks. Words should be remained in the results of join operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jun 26 13:39:13 2018\n",
    "\n",
    "@author: mundanemohit\n",
    "\"\"\"\n",
    "\n",
    "# Load libraries\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, datediff, to_timestamp, unix_timestamp, to_date\n",
    "from pyspark.sql.functions import when, col, round\n",
    "from pyspark.sql.functions import max as max_, min as min_\n",
    "from pyspark.sql.functions import explode, split\n",
    "from pyspark.sql.functions import stddev, mean as mean_, count as count_\n",
    "from pyspark.sql.functions import dense_rank, udf\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "path = '/Users/mundanemohit/Google Drive/My Work/MSc. Data Analytics/3114 Distributed Data Analytics/Excercises/Ex09/'\n",
    "\n",
    "# Create a spark session\n",
    "spark = SparkSession \\\n",
    " .builder \\\n",
    " .appName(\"Python Spark - Testing the Waters\") \\\n",
    " .config(\"spark.some.config.option\", \"some-value\") \\\n",
    " .getOrCreate()\n",
    " \n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Define data as a list\n",
    "a =[\"spark\", \"rdd\", \"python\", \"context\", \"create\", \"class\"]\n",
    "b = [\"operation\", \"apache\", \"scala\", \"lambda\",\"parallel\",\"partition\"]\n",
    "\n",
    "# Convert list to RDDs\n",
    "rddA = sc.parallelize(a)\n",
    "rddB = sc.parallelize(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A1. Perform **rightOuterJoin** and **fullOuterJoin** operations between a and b. Briefly explain your solution. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIGHT OUTER JOIN: \n",
      " [('scala', (None, 'b')), ('parallel', (None, 'b')), ('operation', (None, 'b')), ('apache', (None, 'b')), ('partition', (None, 'b')), ('lambda', (None, 'b'))] \n",
      "\n",
      "FULL OUTER JOIN: \n",
      " [('python', ('a', None)), ('spark', ('a', None)), ('context', ('a', None)), ('create', ('a', None)), ('scala', (None, 'b')), ('parallel', (None, 'b')), ('operation', (None, 'b')), ('apache', (None, 'b')), ('partition', (None, 'b')), ('lambda', (None, 'b')), ('class', ('a', None)), ('rdd', ('a', None))]\n"
     ]
    }
   ],
   "source": [
    "# Add key pair for joining\n",
    "distA = rddA.map(lambda word:(word,'a'))\n",
    "distB = rddB.map(lambda word:(word,'b'))\n",
    "\n",
    "# RightOuterJoin\n",
    "rjoin = distA.rightOuterJoin(distB).collect()\n",
    "print(\"RIGHT OUTER JOIN: \\n\", rjoin, \"\\n\")\n",
    "\n",
    "# FullOuterJoin\n",
    "fjoin = distA.fullOuterJoin(distB).collect()\n",
    "print(\"FULL OUTER JOIN: \\n\",fjoin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2. Using **map** and **reduce** functions to count how many times the character \"s\" appears in all a and b. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of 's' in the list:  4\n"
     ]
    }
   ],
   "source": [
    "# Count S in A & B\n",
    "s_countA = rddA.union(rddB) \\\n",
    "                .flatMap(lambda word:list(word)) \\\n",
    "                .map(lambda x:x.count('s')) \\\n",
    "                .reduce(lambda x,y:x+y)\n",
    "print(\"No. of 's' in the list: \", s_countA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A.3 Using **aggregate** function to count how many times the character \"s\" appears in all a and b. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Count of 's', Total letters) =  (4, 75)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate S in A & B\n",
    "s_countA = rddA.union(rddB) \\\n",
    "                .flatMap(lambda word:list(word)) \\\n",
    "                .map(lambda x:x.count('s')) \\\n",
    "                .aggregate((0,0), \n",
    "                           seqOp = lambda agg, x: (agg[0] + x, agg[1] + 1),\n",
    "                           combOp = lambda agg, x: (agg[0] + x[0], agg[1] + x[1]))\n",
    "print(\"(Count of 's', Total letters) = \", s_countA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part b) Basic Operations on DataFrames (6 points)\n",
    "\n",
    "Use dataset students.json (download from learnweb) for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- course: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- points: long (nullable = true)\n",
      " |-- s_id: long (nullable = true)\n",
      "\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|     null|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|  null|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|              null|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|     null|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DATAFRAMES\n",
    "df = spark.read.json(path + 'students.json')\n",
    "\n",
    "df.printSchema()    # Show structure of DataFrame\n",
    "df.show()          # Show first few datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First creating DataFrames from the dataset and do several tasks as follows:\n",
    "\n",
    "**B1. Replace the null value(s) in column points by the mean of all points. (0.5 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|     null|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    11|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|              null|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|     null|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace null value in points with mean\n",
    "df = df.fillna(df.agg({'points': 'mean'}).collect()[0][0], 'points')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B2. Replace the null value(s) in column dob and column last name by \"unknown\" and \"--\" respectively. (0.5 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|       --|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    11|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|           unknown|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|       --|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace null value in dob with 'unknown'\n",
    "df = df.fillna('unknown', 'dob')\n",
    "\n",
    "# Replace null value in last_name with '--'\n",
    "df = df.fillna('--', 'last_name')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B3. In the dob column, there exist several formats of dates, e.g. October 14, 1983 and 26 December 1989. Let’s convert all the dates into DD-MM-YYYY format where DD, MM and YYYY are two digits for day, two digits for months and four digits for year respectively. (2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+----------+---------+------+----+\n",
      "|            course|       dob|first_name|last_name|points|s_id|\n",
      "+------------------+----------+----------+---------+------+----+\n",
      "|Humanities and Art|1983-10-14|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|1980-09-26|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|1982-06-12|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|1987-04-05|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|1978-11-01|      Kira| Schommer|    11|   5|\n",
      "|          Business|1981-02-17| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|1984-01-01|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|1978-01-13|      John|       --|    10|   8|\n",
      "|  Machine Learning|1989-12-26|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|1987-12-30|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|1975-06-12|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|1985-07-02|     April|    Black|    11|  12|\n",
      "|  Computer Science|1980-07-22|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|1986-02-07|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|1987-05-18|     Rosie|   Norman|     9|  15|\n",
      "|          Business|1984-08-10|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|1990-12-16|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|      null|   Bridget|    Twain|     6|  18|\n",
      "|          Business|1980-03-07|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|1985-06-02|   Zachary|       --|    10|  20|\n",
      "+------------------+----------+----------+---------+------+----+\n",
      "\n",
      "root\n",
      " |-- course: string (nullable = true)\n",
      " |-- dob: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = false)\n",
      " |-- points: long (nullable = true)\n",
      " |-- s_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create new column by transforming dates\n",
    "xdf = df.select('*', to_timestamp(df.dob, 'MMMMM dd, yyyy').alias('date'))\n",
    "#xdf.show()\n",
    "xdf = xdf.withColumn('date', when(col('date').isNull(), to_timestamp(df.dob, 'dd MMMMM yyyy')).otherwise(col('date')))\n",
    "#xdf.show()\n",
    "# xdf = xdf.withColumn('date', when(col('date').isNull(), '1000-01-01 00:00:00').otherwise(col('date')))\n",
    "# xdf.show()\n",
    "xdf = xdf.withColumn('dob', unix_timestamp(col(\"date\"), 'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\"))\n",
    "#xdf.show()\n",
    "#xdf.printSchema()\n",
    "xdf = xdf.drop('date')\n",
    "xdf = xdf.withColumn('dob', to_date('dob', 'dd-MM-yyyy'))\n",
    "xdf.show()\n",
    "xdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B4. Insert a new column age and calculate the current age of all students. (1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+----------+---------+------+----+----+\n",
      "|            course|       dob|first_name|last_name|points|s_id| age|\n",
      "+------------------+----------+----------+---------+------+----+----+\n",
      "|Humanities and Art|1983-10-14|      Alan|      Joe|    10|   1|35.0|\n",
      "|  Computer Science|1980-09-26|    Martin|  Genberg|    17|   2|38.0|\n",
      "|    Graphic Design|1982-06-12|     Athur|   Watson|    16|   3|36.0|\n",
      "|    Graphic Design|1987-04-05|  Anabelle|  Sanberg|    12|   4|31.0|\n",
      "|        Psychology|1978-11-01|      Kira| Schommer|    11|   5|40.0|\n",
      "|          Business|1981-02-17| Christian|   Kiriam|    10|   6|37.0|\n",
      "|  Machine Learning|1984-01-01|   Barbara|  Ballard|    14|   7|35.0|\n",
      "|     Deep Learning|1978-01-13|      John|       --|    10|   8|40.0|\n",
      "|  Machine Learning|1989-12-26|    Marcus|   Carson|    15|   9|29.0|\n",
      "|           Physics|1987-12-30|     Marta|   Brooks|    11|  10|31.0|\n",
      "|    Data Analytics|1975-06-12|     Holly| Schwartz|    12|  11|43.0|\n",
      "|  Computer Science|1985-07-02|     April|    Black|    11|  12|33.0|\n",
      "|  Computer Science|1980-07-22|     Irene|  Bradley|    13|  13|38.0|\n",
      "|        Psychology|1986-02-07|      Mark|    Weber|    12|  14|32.0|\n",
      "|       Informatics|1987-05-18|     Rosie|   Norman|     9|  15|31.0|\n",
      "|          Business|1984-08-10|    Martin|   Steele|     7|  16|34.0|\n",
      "|  Machine Learning|1990-12-16|     Colin| Martinez|     9|  17|28.0|\n",
      "|    Data Analytics|      null|   Bridget|    Twain|     6|  18|null|\n",
      "|          Business|1980-03-07|   Darlene|    Mills|    19|  19|38.0|\n",
      "|    Data Analytics|1985-06-02|   Zachary|       --|    10|  20|33.0|\n",
      "+------------------+----------+----------+---------+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add age column\n",
    "xdf = xdf.select('*', round(datediff(lit(datetime.date.today()), col('dob'))/365.25,0).alias('age'))\n",
    "xdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B5. Let’s consider granting some points for good performed students in the class. For each student, if his point is larger than 1 standard deviation of all points, then we update his current point to 20, which is the maximum. See Annex 1 for a tutorial on how to calculate standard deviation. (2 points)**\n",
    "\n",
    "**B6. Create a histogram on the new points created in the task 5. (1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+----------+---------+------+----+----+\n",
      "|            course|       dob|first_name|last_name|points|s_id| age|\n",
      "+------------------+----------+----------+---------+------+----+----+\n",
      "|Humanities and Art|1983-10-14|      Alan|      Joe|    10|   1|35.0|\n",
      "|  Computer Science|1980-09-26|    Martin|  Genberg|    20|   2|38.0|\n",
      "|    Graphic Design|1982-06-12|     Athur|   Watson|    20|   3|36.0|\n",
      "|    Graphic Design|1987-04-05|  Anabelle|  Sanberg|    12|   4|31.0|\n",
      "|        Psychology|1978-11-01|      Kira| Schommer|    11|   5|40.0|\n",
      "|          Business|1981-02-17| Christian|   Kiriam|    10|   6|37.0|\n",
      "|  Machine Learning|1984-01-01|   Barbara|  Ballard|    14|   7|35.0|\n",
      "|     Deep Learning|1978-01-13|      John|       --|    10|   8|40.0|\n",
      "|  Machine Learning|1989-12-26|    Marcus|   Carson|    20|   9|29.0|\n",
      "|           Physics|1987-12-30|     Marta|   Brooks|    11|  10|31.0|\n",
      "|    Data Analytics|1975-06-12|     Holly| Schwartz|    12|  11|43.0|\n",
      "|  Computer Science|1985-07-02|     April|    Black|    11|  12|33.0|\n",
      "|  Computer Science|1980-07-22|     Irene|  Bradley|    13|  13|38.0|\n",
      "|        Psychology|1986-02-07|      Mark|    Weber|    12|  14|32.0|\n",
      "|       Informatics|1987-05-18|     Rosie|   Norman|     9|  15|31.0|\n",
      "|          Business|1984-08-10|    Martin|   Steele|     7|  16|34.0|\n",
      "|  Machine Learning|1990-12-16|     Colin| Martinez|     9|  17|28.0|\n",
      "|    Data Analytics|      null|   Bridget|    Twain|     6|  18|null|\n",
      "|          Business|1980-03-07|   Darlene|    Mills|    20|  19|38.0|\n",
      "|    Data Analytics|1985-06-02|   Zachary|       --|    10|  20|33.0|\n",
      "+------------------+----------+----------+---------+------+----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ea642b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Update points if they are greater than given benchmark\n",
    "plt.hist(xdf.select('points').rdd.map(lambda row : row.points).collect(), label = 'Before')   # Before update\n",
    "# Calculate benchmark\n",
    "benchmark = xdf.agg({'points': 'mean'}).collect()[0][0] + xdf.agg({'points': 'stddev'}).collect()[0][0]\n",
    "# Update\n",
    "xdf = xdf.withColumn('points', when(col('points') > benchmark, 20).otherwise(col('points')))\n",
    "xdf.show()\n",
    "# Plot points\n",
    "plt.hist(xdf.select('points').rdd.map(lambda row : row.points).collect(), label = 'after')  # After update\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Manipulating Recommender Dataset with Apache Spark (10 points)\n",
    "\n",
    "A tagging session for a user can be defined as the duration in which he/she generated tagging activities. Typically, an inactive duration of 30 mins is considered as a termination of the tagging session. \n",
    "\n",
    "**1. Your task is to separate out tagging sessions for each user.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "The task is broken down into following steps:\n",
    "\n",
    "* Read Data\n",
    "* Remove junk columns\n",
    "* Rename columns\n",
    "* Convert timestamp column to correct format\n",
    "* Remove any duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------------+-------------------+\n",
      "|UserID|MovieID|            Tag|          Timestamp|\n",
      "+------+-------+---------------+-------------------+\n",
      "|   146|   1028|          magic|2008-03-23 06:54:34|\n",
      "|   146|   1059|based on a play|2008-03-29 10:19:24|\n",
      "|   146|   2927|    Noel Coward|2008-11-15 11:32:08|\n",
      "|   146|   3335|imdb bottom 100|2007-11-21 04:56:51|\n",
      "|   146|   3653|    New Zealand|2008-12-05 05:55:54|\n",
      "|   146|   4339|         trains|2007-11-02 01:08:12|\n",
      "|   146|   5045|imdb bottom 100|2007-11-21 05:08:48|\n",
      "|   146|   6483|imdb bottom 100|2007-11-21 04:41:56|\n",
      "|   146|   6672|     journalism|2007-12-07 04:49:02|\n",
      "|   146|  48738|         Africa|2007-12-14 11:43:16|\n",
      "+------+-------+---------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- UserID: string (nullable = true)\n",
      " |-- MovieID: string (nullable = true)\n",
      " |-- Tag: string (nullable = true)\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "95373"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Users/mundanemohit/Google Drive/My Work/MSc. Data Analytics/3114 Distributed Data Analytics/Excercises/Ex09/'\n",
    "file = '/ml-10M100K/tags.dat'\n",
    "\n",
    "# Read movie data into python\n",
    "tagDF = spark.read.csv(path+file, sep=':')\n",
    "\n",
    "# Remove junk columns\n",
    "tagDF = tagDF.drop('_c1', '_c3', '_c5')\n",
    "\n",
    "# Rename columns\n",
    "tagDF = tagDF.selectExpr('_c0 as UserID', '_c2 as MovieID', '_c4 as Tag', '_c6 as Timestamp')\n",
    "\n",
    "# Modify timestamp column\n",
    "tagDF = tagDF.withColumn(\"Timestamp\", col('Timestamp').cast(\"double\").cast(\"timestamp\"))  \n",
    "\n",
    "# Remove duplicate records\n",
    "tagDF = tagDF.distinct()\n",
    "\n",
    "tagDF.show(10)\n",
    "tagDF.printSchema()\n",
    "tagDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further next steps are as follows:\n",
    "\n",
    "* Identify order of tagging for each user and mark as 'Rank'\n",
    "* Map next item on list to the current one to calculate time difference\n",
    "* Use time difference to identify whether a tag belongs to a session or not\n",
    "* Flag all tags which belong to a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------+-------------------+----+-------------------+---------+----+\n",
      "|UserID|MovieID|           Tag|          Timestamp|Rank|        nextag_time|time_diff|flag|\n",
      "+------+-------+--------------+-------------------+----+-------------------+---------+----+\n",
      "| 11563|  37830| final fantasy|2007-10-15 14:58:42|   1|               null|     null|   0|\n",
      "|  1436|    838|       cottage|2007-09-02 18:44:43|   1|2007-09-02 18:45:02|       19|   1|\n",
      "|  1436|   1953|action classic|2007-09-02 18:45:02|   2|2007-09-02 18:46:17|       75|   1|\n",
      "|  1436|   1231|       awesome|2007-09-02 18:46:17|   3|2007-09-02 18:46:20|        3|   1|\n",
      "|  1436|    247| coming of age|2007-09-02 18:46:20|   4|2007-09-02 18:46:34|       14|   1|\n",
      "|  1436|   1994|           80s|2007-09-02 18:46:34|   5|2007-09-02 18:47:04|       30|   1|\n",
      "|  1436|   1179|     new  nior|2007-09-02 18:47:04|   6|2007-09-02 18:47:26|       22|   1|\n",
      "|  1436|  32587|    comic book|2007-09-02 18:47:26|   7|2007-09-02 18:47:48|       22|   1|\n",
      "|  1436|   6378|        action|2007-09-02 18:47:48|   8|2007-09-02 18:47:57|        9|   1|\n",
      "|  1436|   4447|        comedy|2007-09-02 18:47:57|   9|2007-09-02 18:48:28|       31|   1|\n",
      "+------+-------+--------------+-------------------+----+-------------------+---------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find rank of tags\n",
    "w =  Window.partitionBy(tagDF['UserID']).orderBy(tagDF['TimeStamp'].asc(), tagDF['MovieID'].asc(), tagDF['tag'].asc())\n",
    "tagDF = tagDF.withColumn(\"Rank\", dense_rank().over(w))\n",
    "\n",
    "# Get next tagging time\n",
    "tagDF.createOrReplaceTempView(\"tags\")\n",
    "tagDF = spark.sql(\"SELECT a.*, b.Timestamp as nextag_time                              \\\n",
    "                   FROM tags a                                                         \\\n",
    "                   left join tags b                                                    \\\n",
    "                          on a.UserID = b.UserID                                       \\\n",
    "                         and a.Rank = b.Rank - 1\")\n",
    "\n",
    "# Get time difference in consecutive tags\n",
    "tagDF = tagDF.withColumn('time_diff', unix_timestamp('nextag_time') - unix_timestamp('Timestamp'))\n",
    "\n",
    "# flag if tag is part of a session\n",
    "tagDF = tagDF.withColumn(\"flag\", when(col('time_diff') < 1800, 1).otherwise(0))\n",
    "tagDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a **user-defined function** to identify each unique tagging session for a user.\n",
    "\n",
    "* Map flag of previous tag to current tag\n",
    "* If current tag belongs to previous session, we assign the same session id\n",
    "* If not, we increment the session id by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map previous flag\n",
    "tagDF.createOrReplaceTempView(\"tags\")\n",
    "a = tagDF.alias('a')\n",
    "b = tagDF.alias('b')\n",
    "\n",
    "tagDF = spark.sql(\"SELECT a.*, b.flag as prflag        \\\n",
    "                   FROM tags a                         \\\n",
    "                   left join tags b                    \\\n",
    "                          on a.UserID = b.UserID       \\\n",
    "                         and a.Rank = b.Rank + 1       \\\n",
    "                   order by a.userid, a.rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------------+-------------------+----+---------+\n",
      "|UserID|MovieID|             Tag|          Timestamp|Rank|SessionID|\n",
      "+------+-------+----------------+-------------------+----+---------+\n",
      "|  1000|    277|children's story|2007-08-31 06:05:11|   1|        1|\n",
      "|  1000|   1994|    sci-fi. dark|2007-08-31 06:05:36|   2|        1|\n",
      "|  1000|   5377|         romance|2007-08-31 06:05:50|   3|        1|\n",
      "|  1000|   7147|    family bonds|2007-08-31 06:06:01|   4|        1|\n",
      "|  1000|    362|animated classic|2007-08-31 06:06:11|   5|        1|\n",
      "|  1000|    276|          family|2007-08-31 06:07:15|   6|        1|\n",
      "| 10003|  42013|        Passable|2006-06-16 06:33:55|   1|        1|\n",
      "| 10003|  51662|  FIOS on demand|2008-04-12 00:35:26|   2|        2|\n",
      "| 10003|  54997|  FIOS on demand|2008-04-12 00:35:35|   3|        2|\n",
      "| 10003|  55765|  FIOS on demand|2008-04-12 00:35:42|   4|        2|\n",
      "| 10003|  55363|  FIOS on demand|2008-04-12 00:37:00|   5|        2|\n",
      "| 10003|  56152|  FIOS on demand|2008-04-12 00:38:46|   6|        2|\n",
      "| 10003|  55116|  FIOS on demand|2008-04-12 00:40:36|   7|        2|\n",
      "| 10003|  56174|  FIOS on demand|2008-04-12 00:41:10|   8|        2|\n",
      "| 10003|  55176|  FIOS on demand|2008-04-12 00:42:35|   9|        2|\n",
      "| 10003|  55247|  FIOS on demand|2008-04-12 00:42:36|  10|        2|\n",
      "| 10003|  54881|  FIOS on demand|2008-04-12 00:42:38|  11|        2|\n",
      "| 10003|  55820|  FIOS on demand|2008-04-12 00:44:33|  12|        2|\n",
      "| 10003|  53123|  FIOS on demand|2008-04-12 00:44:35|  13|        2|\n",
      "| 10003|  53550|  FIOS on demand|2008-04-12 00:45:37|  14|        2|\n",
      "| 10003|  54745|  FIOS on demand|2008-04-12 00:45:40|  15|        2|\n",
      "| 10003|  54259|  FIOS on demand|2008-04-12 00:46:50|  16|        2|\n",
      "| 10003|  52328|  FIOS on demand|2008-04-12 00:46:52|  17|        2|\n",
      "| 10003|  55286|  FIOS on demand|2008-04-12 00:47:52|  18|        2|\n",
      "| 10003|  53996|  FIOS on demand|2008-04-12 00:48:02|  19|        2|\n",
      "| 10003|   5709|            hulu|2008-11-02 05:17:38|  20|        3|\n",
      "| 10003|   6458|            hulu|2008-11-02 05:20:12|  21|        3|\n",
      "| 10003|   2517|            hulu|2008-11-02 05:23:33|  22|        3|\n",
      "| 10003|   7460|            hulu|2008-11-02 05:24:24|  23|        3|\n",
      "| 10003|  25835|            hulu|2008-11-02 05:26:15|  24|        3|\n",
      "| 10003|   5406|            hulu|2008-11-02 05:26:43|  25|        3|\n",
      "| 10003|    451|            hulu|2008-11-02 05:29:07|  26|        3|\n",
      "| 10003|   6990|            hulu|2008-11-02 05:31:39|  27|        3|\n",
      "| 10003|   4923|            hulu|2008-11-02 05:32:38|  28|        3|\n",
      "| 10003|    246|            hulu|2008-11-02 05:33:45|  29|        3|\n",
      "| 10003|   1950|            hulu|2008-11-02 05:34:10|  30|        3|\n",
      "| 10003|   3091|            hulu|2008-11-02 05:35:42|  31|        3|\n",
      "| 10003|  26155|            hulu|2008-11-02 05:36:27|  32|        3|\n",
      "| 10003|  32074|            hulu|2008-11-02 05:36:56|  33|        3|\n",
      "| 10003|   1289|            hulu|2008-11-02 05:37:46|  34|        3|\n",
      "| 10003|   3062|            hulu|2008-11-02 05:38:26|  35|        3|\n",
      "| 10003|   1464|            hulu|2008-11-02 05:38:50|  36|        3|\n",
      "| 10003|    600|            hulu|2008-11-02 05:39:17|  37|        3|\n",
      "| 10003|    272|            hulu|2008-11-02 05:39:50|  38|        3|\n",
      "| 10003|   1966|            hulu|2008-11-02 05:40:46|  39|        3|\n",
      "| 10003|   4429|            hulu|2008-11-02 05:41:31|  40|        3|\n",
      "| 10003|   7123|            hulu|2008-11-02 05:42:46|  41|        3|\n",
      "| 10003|   3727|            hulu|2008-11-02 05:43:21|  42|        3|\n",
      "| 10003|   7013|            hulu|2008-11-02 05:43:48|  43|        3|\n",
      "| 10003|  39886|            hulu|2008-11-02 05:44:13|  44|        3|\n",
      "+------+-------+----------------+-------------------+----+---------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#global variable\n",
    "count = 1\n",
    "# For tagging\n",
    "def tagSession(flag, prflag):\n",
    "    global count\n",
    "    # First entry  --> return 1\n",
    "    if prflag is None:\n",
    "        count = 1\n",
    "        return count\n",
    "    # Same session -----> return same session ID\n",
    "    if prflag == 1:\n",
    "        return count\n",
    "    # New session  ----> increment session id\n",
    "    elif prflag == 0:\n",
    "        count = count + 1\n",
    "        return count\n",
    "    # Unknown possibility\n",
    "    else:   \n",
    "        return -1\n",
    "\n",
    "# Create UDF\n",
    "sessUDF = udf(tagSession)\n",
    "\n",
    "# Sort dataset\n",
    "tagDF = tagDF.sort(\"UserID\", \"Rank\")\n",
    "\n",
    "# Create session IDs\n",
    "tagDF = tagDF.withColumn('SessionID', sessUDF(\"flag\", \"prflag\"))\n",
    "\n",
    "# Drop additional columns\n",
    "tagDF = tagDF.drop('nextag_time', 'flag', 'prflag', 'time_diff')\n",
    "\n",
    "tagDF.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have successfully identified each user session and are ready to start with the other tasks.\n",
    "\n",
    "**2. Calculate the frequency of tagging for each user session.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+\n",
      "|UserID|SessionID|num_tags|\n",
      "+------+---------+--------+\n",
      "|  1000|        1|       6|\n",
      "| 10003|        1|       1|\n",
      "| 10003|        2|      18|\n",
      "| 10003|        3|      38|\n",
      "| 10020|        1|       2|\n",
      "| 10025|        1|       1|\n",
      "| 10032|        1|      39|\n",
      "| 10032|       10|       1|\n",
      "| 10032|       11|       1|\n",
      "| 10032|       12|       1|\n",
      "+------+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summDF = tagDF.groupby(['UserID', 'SessionID']).count()                         \\\n",
    "              .select('UserID', 'SessionID', col('count').alias('num_tags'))    \\\n",
    "              .sort(\"UserID\", \"SessionID\").cache()                                      \\\n",
    "\n",
    "summDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Find a mean and standard deviation of the tagging frequency of each user.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------------+------------------+\n",
      "|UserID|avg_tags_per_session|num_sessions|           std_dev|\n",
      "+------+--------------------+------------+------------------+\n",
      "|  1000|                 6.0|           1|               NaN|\n",
      "| 10003|                19.0|           3|18.520259177452136|\n",
      "| 10020|                 2.0|           1|               NaN|\n",
      "| 10025|                 1.0|           1|               NaN|\n",
      "| 10032|   4.666666666666667|          12|10.873933246182093|\n",
      "| 10051|                 1.0|           1|               NaN|\n",
      "| 10058|  25.333333333333332|           3|15.044378795195676|\n",
      "| 10059|                 2.5|           2|0.7071067811865476|\n",
      "| 10064|                 1.0|           1|               NaN|\n",
      "| 10084|                3.75|           4|2.0615528128088303|\n",
      "+------+--------------------+------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userDF =  summDF.groupby('UserID')                                  \\\n",
    "                .agg(mean_('num_tags').alias('avg_tags_per_session')\n",
    "                    ,count_(lit(1)).alias('num_sessions')      \n",
    "                    ,stddev('num_tags').alias('std_dev'))           \\\n",
    "                .sort('UserID')                           \n",
    "\n",
    "userDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Find a mean and standard deviation of the tagging frequency for across users.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+------------------+\n",
      "|        avg_tags|num_session|           std_dev|\n",
      "+----------------+-----------+------------------+\n",
      "|8.21331381329659|      11612|30.589354451796858|\n",
      "+----------------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "totDF = summDF.agg(mean_('num_tags').alias('avg_tags')\n",
    "                  ,count_(lit(1)).alias('num_session')      \n",
    "                  ,stddev('num_tags').alias('std_dev'))     \n",
    "\n",
    "totDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Provide the list of users with a mean tagging frequency within the two standard deviation from the mean frequency of all users.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|UserID|avg_tags_per_session|\n",
      "+------+--------------------+\n",
      "|  1000|                 6.0|\n",
      "| 10003|                19.0|\n",
      "| 10020|                 2.0|\n",
      "| 10025|                 1.0|\n",
      "| 10032|   4.666666666666667|\n",
      "| 10051|                 1.0|\n",
      "| 10058|  25.333333333333332|\n",
      "| 10059|                 2.5|\n",
      "| 10064|                 1.0|\n",
      "| 10084|                3.75|\n",
      "+------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean    = totDF.collect()[0]['avg_tags']\n",
    "std_dev = totDF.collect()[0]['std_dev'] \n",
    "\n",
    "\n",
    "userDF.filter((col('avg_tags_per_session') >= (mean - (2*std_dev)))  & \n",
    "              (col('avg_tags_per_session') <= (mean + (2*std_dev)))) \\\n",
    "     .select('UserID','avg_tags_per_session')                       \\\n",
    "     .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus (Optional Question): Analysis of Movie dataset using Apache Spark MapReduce (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-------------------+\n",
      "|UserID|MovieID|Rating|          Timestamp|\n",
      "+------+-------+------+-------------------+\n",
      "| 10005|   4734|     3|2004-07-07 09:22:50|\n",
      "| 10007|   2867|     4|2000-02-03 16:53:11|\n",
      "| 10008|   2840|     2|2000-11-20 00:10:39|\n",
      "| 10011|    196|     2|1996-08-11 17:47:56|\n",
      "| 10012|   5387|     3|2005-08-06 05:48:11|\n",
      "+------+-------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- UserID: string (nullable = true)\n",
      " |-- MovieID: string (nullable = true)\n",
      " |-- Rating: string (nullable = true)\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      "\n",
      "+-------+--------------------+--------------------+\n",
      "|MovieID|               Title|              Genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|    432|    City Slickers II|                null|\n",
      "|    484|       Lassie (1994)|  Adventure|Children|\n",
      "|    762|   Striptease (1996)|        Comedy|Crime|\n",
      "|   1032|Alice in Wonderla...|Adventure|Animati...|\n",
      "|   1125|Return of the Pin...|        Comedy|Crime|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- MovieID: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Genres: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/mundanemohit/Google Drive/My Work/MSc. Data Analytics/3114 Distributed Data Analytics/Excercises/Ex09/ml-10M100K/'\n",
    "\n",
    "# Read movie data into python\n",
    "ratingDF = spark.read.csv(path + 'ratings.dat', sep=':')\n",
    "movieDF = spark.read.csv(path + 'movies.dat', sep=':')\n",
    "\n",
    "# Preprocess ratings dataset\n",
    "ratingDF = ratingDF.drop('_c1', '_c3', '_c5')\n",
    "# Rename columns\n",
    "ratingDF = ratingDF.selectExpr('_c0 as UserID', '_c2 as MovieID', '_c4 as Rating', '_c6 as Timestamp')\n",
    "# Modify timestamp column\n",
    "ratingDF = ratingDF.withColumn(\"Timestamp\", col('Timestamp').cast(\"double\").cast(\"timestamp\"))  \n",
    "ratingDF = ratingDF.distinct().cache()\n",
    "ratingDF.show(5)\n",
    "ratingDF.printSchema()\n",
    "\n",
    "# Preprocess movies dataset\n",
    "movieDF = movieDF.drop('_c1', '_c3')\n",
    "# Rename columns\n",
    "movieDF = movieDF.selectExpr('_c0 as MovieID', '_c2 as Title', '_c4 as Genres')\n",
    "# Modify timestamp column\n",
    "movieDF = movieDF.distinct().cache()\n",
    "movieDF.show(5)\n",
    "movieDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Find the movie title which has the maximum average ratings?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+\n",
      "|MovieID|               Title|avg_rating|\n",
      "+-------+--------------------+----------+\n",
      "|  42783|Shadows of Forgot...|       5.0|\n",
      "|  51209|Fighting Elegy (K...|       5.0|\n",
      "|  33264|Satan's Tango (Sá...|       5.0|\n",
      "|  64275|Blue Light, The (...|       5.0|\n",
      "|  53355|Sun Alley (Sonnen...|       5.0|\n",
      "+-------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find average ratings for all movies\n",
    "movieSummDF = ratingDF.groupby('MovieID')                                   \\\n",
    "                      .agg(mean_('Rating').alias('avg_rating'))             \\\n",
    "                      .join(movieDF, ratingDF.MovieID == movieDF.MovieID)   \\\n",
    "                      .select(movieDF.MovieID,'Title', 'avg_rating')        \n",
    "\n",
    "# Find highest rated movies\n",
    "movieSummDF.filter(col('avg_rating') == \n",
    "                   movieSummDF.agg(max_('avg_rating'))                      \\\n",
    "                              .collect()[0][0]).show(10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 movies with an average rating of 5.\n",
    "\n",
    "\n",
    "**2. Find the user who has assign the lowest average ratings among all the users the number of ratings greater than 40**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------------+\n",
      "|UserID|avg_rating|total_ratings|\n",
      "+------+----------+-------------+\n",
      "| 24176|       1.0|          147|\n",
      "+------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find avg. ratings for each user\n",
    "userSummDF = ratingDF.groupby('UserID')                                     \\\n",
    "                      .agg(mean_('Rating').alias('avg_rating'),\n",
    "                           count_(lit(1)).alias('total_ratings'))           \\\n",
    "                      .filter('total_ratings >= 40')\n",
    "\n",
    "# Find user with lowest avg. ratings\n",
    "min_rating =  userSummDF.agg(min_('avg_rating'))   \n",
    "\n",
    "userSummDF.filter(col('avg_rating') == min_rating.collect()[0][0])          \\\n",
    "          .show(5)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aforementioned user has an average rating of 1, and has rated 147 movie titles.\n",
    "\n",
    "\n",
    "**3. Find the movie genre with the highest average ratings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Genres='Film-Noir', avg_rating=4.0138814367402)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find movie genre with highest average ratings\n",
    "Genrelist = ['Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', \n",
    "             'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical',\n",
    "             'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "genreSummDF = ratingDF.join(movieDF, ratingDF.MovieID == movieDF.MovieID)   \\\n",
    "                      .select(movieDF.Genres, 'Rating')             \n",
    "\n",
    "genreSummDF.withColumn(\"Genres\", explode(split('Genres', \"[|]\")))           \\\n",
    "           .filter(col('Genres').isin(Genrelist))                           \\\n",
    "           .groupBy('Genres')                                               \\\n",
    "           .agg(mean_('Rating').alias('avg_rating'))                        \\\n",
    "           .orderBy(col('avg_rating').desc())                               \\\n",
    "           .take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Film-noir** genre has the highest average rating of 4.01"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
